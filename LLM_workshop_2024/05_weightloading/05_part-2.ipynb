{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0599d57c-16e4-478c-954d-89dbbd193ced",
   "metadata": {},
   "source": [
    "**LLM Workshop 2024 by Sebastian Raschka**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b82151-07c6-4867-b374-741258033b52",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 5) Loading pretrained weights (part 2; using LitGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d617b8f-8493-4afa-8c91-f3d1ab79795b",
   "metadata": {},
   "source": [
    "- Now, we are loading the weights using an open-source library called LitGPT\n",
    "- LitGPT is fundamentally similar to the LLM code we implemented previously, but it is much more sophisticated and supports more than 20 different LLMs (Mistral, Gemma, Llama, Phi, and more)\n",
    "\n",
    "# ⚡ LitGPT\n",
    "\n",
    "**20+ high-performance LLMs with recipes to pretrain, finetune, deploy at scale.**\n",
    "\n",
    "<pre>\n",
    "✅ From scratch implementations     ✅ No abstractions    ✅ Beginner friendly   \n",
    "✅ Flash attention                  ✅ FSDP               ✅ LoRA, QLoRA, Adapter\n",
    "✅ Reduce GPU memory (fp4/8/16/32)  ✅ 1-1000+ GPUs/TPUs  ✅ 20+ LLMs            \n",
    "</pre>\n",
    "\n",
    "## Basic usage:\n",
    "\n",
    "```\n",
    "# ligpt [action] [model]\n",
    "litgpt  download  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  chat      meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  evaluate  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  finetune  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  pretrain  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  serve     meta-llama/Meta-Llama-3-8B-Instruct\n",
    "```\n",
    "\n",
    "\n",
    "- You can learn more about LitGPT in the [corresponding GitHub repository](https://github.com/Lightning-AI/litgpt), that contains many tutorials, use cases, and examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f9508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install litgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cf71fa-af17-4c72-a6ab-f258a2b5a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "litgpt version: 0.5.0\n",
      "torch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"litgpt\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29baa9-c3b0-493d-94b4-eaa8146d6b3c",
   "metadata": {},
   "source": [
    "- First, let's see what LLMs are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae8df66-f391-4266-b437-a1f601a6ac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uvloop is not installed. Falling back to the default asyncio event loop. Please install uvloop for better performance using `pip install uvloop`.\n",
      "uvloop is not installed. Falling back to the default asyncio event loop.\n",
      "Please specify --repo_id <repo_id>. Available values:\n",
      "codellama/CodeLlama-13b-hf\n",
      "codellama/CodeLlama-13b-Instruct-hf\n",
      "codellama/CodeLlama-13b-Python-hf\n",
      "codellama/CodeLlama-34b-hf\n",
      "codellama/CodeLlama-34b-Instruct-hf\n",
      "codellama/CodeLlama-34b-Python-hf\n",
      "codellama/CodeLlama-70b-hf\n",
      "codellama/CodeLlama-70b-Instruct-hf\n",
      "codellama/CodeLlama-70b-Python-hf\n",
      "codellama/CodeLlama-7b-hf\n",
      "codellama/CodeLlama-7b-Instruct-hf\n",
      "codellama/CodeLlama-7b-Python-hf\n",
      "databricks/dolly-v2-12b\n",
      "databricks/dolly-v2-3b\n",
      "databricks/dolly-v2-7b\n",
      "EleutherAI/pythia-1.4b\n",
      "EleutherAI/pythia-1.4b-deduped\n",
      "EleutherAI/pythia-12b\n",
      "EleutherAI/pythia-12b-deduped\n",
      "EleutherAI/pythia-14m\n",
      "EleutherAI/pythia-160m\n",
      "EleutherAI/pythia-160m-deduped\n",
      "EleutherAI/pythia-1b\n",
      "EleutherAI/pythia-1b-deduped\n",
      "EleutherAI/pythia-2.8b\n",
      "EleutherAI/pythia-2.8b-deduped\n",
      "EleutherAI/pythia-31m\n",
      "EleutherAI/pythia-410m\n",
      "EleutherAI/pythia-410m-deduped\n",
      "EleutherAI/pythia-6.9b\n",
      "EleutherAI/pythia-6.9b-deduped\n",
      "EleutherAI/pythia-70m\n",
      "EleutherAI/pythia-70m-deduped\n",
      "garage-bAInd/Camel-Platypus2-13B\n",
      "garage-bAInd/Camel-Platypus2-70B\n",
      "garage-bAInd/Platypus-30B\n",
      "garage-bAInd/Platypus2-13B\n",
      "garage-bAInd/Platypus2-70B\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "garage-bAInd/Platypus2-7B\n",
      "garage-bAInd/Stable-Platypus2-13B\n",
      "google/codegemma-7b-it\n",
      "google/gemma-2-27b\n",
      "google/gemma-2-27b-it\n",
      "google/gemma-2-2b\n",
      "google/gemma-2-2b-it\n",
      "google/gemma-2-9b\n",
      "google/gemma-2-9b-it\n",
      "google/gemma-2b\n",
      "google/gemma-2b-it\n",
      "google/gemma-7b\n",
      "google/gemma-7b-it\n",
      "h2oai/h2o-danube2-1.8b-chat\n",
      "keeeeenw/MicroLlama\n",
      "lmsys/longchat-13b-16k\n",
      "lmsys/longchat-7b-16k\n",
      "lmsys/vicuna-13b-v1.3\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-13b-v1.5-16k\n",
      "lmsys/vicuna-33b-v1.3\n",
      "lmsys/vicuna-7b-v1.3\n",
      "lmsys/vicuna-7b-v1.5\n",
      "lmsys/vicuna-7b-v1.5-16k\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "meta-llama/Llama-2-70b-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Llama-3.2-1B\n",
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "meta-llama/Llama-3.2-3B\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-405B\n",
      "meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-70B\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-8B\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "microsoft/phi-1_5\n",
      "microsoft/phi-2\n",
      "microsoft/Phi-3-mini-4k-instruct\n",
      "microsoft/Phi-3.5-mini-instruct\n",
      "mistralai/mathstral-7B-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "mistralai/Mistral-7B-Instruct-v0.3\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-v0.3\n",
      "mistralai/Mistral-Large-Instruct-2407\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "NousResearch/Nous-Hermes-13b\n",
      "NousResearch/Nous-Hermes-llama-2-7b\n",
      "NousResearch/Nous-Hermes-Llama2-13b\n",
      "openlm-research/open_llama_13b\n",
      "openlm-research/open_llama_3b\n",
      "openlm-research/open_llama_7b\n",
      "stabilityai/FreeWilly2\n",
      "stabilityai/stable-code-3b\n",
      "stabilityai/stablecode-completion-alpha-3b\n",
      "stabilityai/stablecode-completion-alpha-3b-4k\n",
      "stabilityai/stablecode-instruct-alpha-3b\n",
      "stabilityai/stablelm-3b-4e1t\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-tuned-alpha-3b\n",
      "stabilityai/stablelm-tuned-alpha-7b\n",
      "stabilityai/stablelm-zephyr-3b\n",
      "tiiuae/falcon-180B\n",
      "tiiuae/falcon-180B-chat\n",
      "tiiuae/falcon-40b\n",
      "tiiuae/falcon-40b-instruct\n",
      "tiiuae/falcon-7b\n",
      "tiiuae/falcon-7b-instruct\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "togethercomputer/RedPajama-INCITE-7B-Base\n",
      "togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1\n",
      "Trelis/Llama-2-7b-chat-hf-function-calling-v2\n",
      "unsloth/Mistral-7B-v0.2\n"
     ]
    }
   ],
   "source": [
    "!litgpt download list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495037e-0068-49ad-9bed-0bcdc440727d",
   "metadata": {},
   "source": [
    "- We can then download an LLM via the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0c202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !litgpt download microsoft/phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf5be0-4aa1-498f-b08a-68ff234cbea5",
   "metadata": {},
   "source": [
    "- And there's also a Python API to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e057edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yrobi\\Desktop\\Robin World\\Data Science - Machine Learning Prep\\01 - Generative AI\\LLMs-from-scratch\\llm_env\\lib\\site-packages\\litgpt\\utils.py:613: UserWarning: The file size of checkpoints\\microsoft\\phi-2\\lit_model.pth is over 4.2 GB. Using a model with more than 1B parameters on a CPU can be slow, it is recommended to switch to a GPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Llamas are herbivores and will eat a variety of plants, including grasses, shrubs, and leaves.  They may also consume certain vegetables and fungi.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(\"microsoft/phi-2\")\n",
    "\n",
    "llm.generate(\"What do Llamas eat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc775d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The correct response for the question 'What do Llamas eat?' is 'Llamas are herbivores and mainly eat grass, plants, and shrubs.'\n"
     ]
    }
   ],
   "source": [
    "result = llm.generate(\"What do Llamas eat?\", stream=True, max_new_tokens=200)\n",
    "for e in result:\n",
    "    print(e, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288158da",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Exercise 2: Download an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717b67f",
   "metadata": {},
   "source": [
    "- Download and try out an LLM of your own choice (recommendation: 7B parameters or smaller)\n",
    "- We will finetune the LLM in the next notebook\n",
    "- You can also try out the `litgpt chat` command from the terminal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
